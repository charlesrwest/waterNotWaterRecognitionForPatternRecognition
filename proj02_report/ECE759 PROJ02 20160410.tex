% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{enumitem}

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{PROJECT 2}%replace X with the appropriate number
\author{ECE759 Pattern Recognition-Spring 2016\\ %replace with your name
Laura Gonzalez\\Charles West\\Selene Schmittling\\} %if necessary, replace with your course title
 
\maketitle

\section{Introduction}

In this project we utilize the dataset used in Project 1.  This data set includes two groups of images: one set are labeled "Water" images and the other is labeled "Non-water" images. In project 1, the images were manually cropped and water pixels labeled for both sets of images. \\


\section{Methodology}
	\subsection{Neural Networks}
\subsubsection{Convolutional Neural Networks}
	Convolutional neural networks deliver state of the art performance in image classification and segmentation, delivering state of the art results with large datasets such as ImageNet.  There are a few key factors that have allowed convolutional neural networks to reach this level of performance.  It is well known that increasing the amount of training data a good method uses delivers increase performance and generalization.  Neural networks are capable of productively utilizing nearly unlimited amounts of training data, primarily constrained by memory and processing speeds.

	Convolutional neural networks are especially well suited to image processing due to their spacial coherence.  Rather than using fully connected neurons which examine the whole image at once (thereby introducing a extremely large number of parameters), convolutional neural networks instead learn filters which are then applied to the full image.  This drastically reduces the number of parameters  per layer of the network and introduces natural translation invariance due to treating each pixel identically (since the filter is applied at every offset in the network).  This approach evolves filters that have been identified as similar to the filters that have been manually produced in computer vision literature via exhaustive analysis, while producing new and interesting filters that had not been previously identified in higher layers.  Since some of these filters/features appear to be universal among images (such as edge detection), it has become a common practice to take layers with pretrained weights from top performing networks and then retrain with other top layers and data sets to create new classifiers.  This practice allows much smaller training sets to be used and still generate good/generalizable results, essentially borrowing features from otherwise unrelated training data.

	Convolution neural networks work extremely well for complex images and scenes but require large amounts of data (unless the layer borrowing method is used), since the network is essentially having to recreate how vision works from scratch when it starts training.  It is normal for the training sets for these networks to be on the order of a million images.  There are roughly 70 training/test images for this project, which implies that convolutional neural networks (and other methods) might have difficulty generalizing.

	One of the key enabling factors for the high performance of modern neural networks is their ability to use high power GPUs to reduce training time by a factor of 100 or more.  This allows the networks to both have high complexity and handle large amounts of data.  However, this GPU dependence also makes it significantly more difficult to setup and train large neural networks because specialized/experimental libraries and expensive hardware required to get fast results.  Effectively, training convolutional neural networks requires a certain amount of software engineering expertise just to get started.

\subsubsection{Neural Network Libraries}
	Two neural network libraries where considered and used for this project:  

TinyCNN is a header only C++ library which advertises its ease of integration/use (minimal dependencies).  It supports feed forward neural networks without skip layers and  multi-threaded CPU based training (but does not allow GPU based training).  It has minimal support for image processing, with one or two examples of how to read data from an OpenCV Mat image class.  

Caffe is a full featured library which focuses on GPU accelerated convolutional neural networks and language independent sharing of trained networks.  It supports feed forward networks with relatively arbitrary architectures (including skip layers) and supports both single CPU training and training on one or more GPUs.  Working with the library revealed that it has a strong focus on image classification which can make it somewhat difficult to use for image segmentation due to some assumptions in the library code.  Caffe has a large number of dependencies and requires recent versions of its dependency libraries.

	Given the limited amount of time available for completing the project, a plan was formed to get some working results using the TinyCNN library and then explore what would be needed to train a more sophisticated network with Caffe since it seemed likely that it would take significant effort to get Caffe working.

\subsubsection{Initial Neural Network Classifier}
	The C++ framework which had been written for project 1 was reused and a new “NeuralNetworkClassifier” class was added.  The framework allows subclasses of the base class “classifierBaseClass” to define “train” and “segment” functions and then automatically presents them with image data.  Since there was some uncertainty with the ordering of the data required to use TinyCNN's convolutional neural network layers (since they have spacial dependence), the images where instead broken up into a set 5x5 chunks (one for each pixel, with its neighbors) and presented to a fully connected neural network.

	The initial network was trained for 24 hours on a Thinkpad laptop and then the results were analyzed.  The segmentations produced by the initial classifier were completely blank.  Tweaking the parameters and retraining didn't produce better results, so it was decided that it would be preferable to start with a system that was easier to debug and then move to the large scale system.

\subsubsection{Network Design Considerations}

	A supervised learning method is essentially a function approximator, so approximating a sine function using a a fully connected neural network defined to be the initial test task.  The shorter training time of this simple case allowed more exploration of how network architectures, activation functions, loss functions, dropout and solver strategies affected results and training time.  There were a number of different methods available in the library but few clear guides in the literature on how to choose between them.

	Sigmoid activation functions were popular for much of the neural network research conducted previous to the advent of “deep learning”.  Recent literature appears to indicate that sigmoid activation functions have theoretical properties which tend to result in a “vanishing gradient” which makes it difficult to train deep sigmoid networks.  It has been suggested that hyperbolic tan based activation functions should nearly always be used in applications in which sigmoid activations where previously employed due to their greater resistance to the vanishing gradient problem.
  
	Rectified linear functions are a more recently developed activation function which has enjoyed considerable success in many areas.  They are essentially the max(input sum, 0), which means that their gradient is very fast to compute (because it is either 0 or 1).  They don't suffer from the vanishing gradient problem.  Their primary drawback (which also been cited as an advantage) is that nodes can “switch off” once their inputs go consistently below 0 and their gradient goes to 0.  This results in sparse networks once training finishes but there are some concerns that nodes could be “switched off” too early in the training.  “Leaky Rectified Linear Units” are a alternative to rectified linear units which maintain a small but finite gradient when they would normally be “switched off”.  This prevents the development of sparse networks.  It has also been claimed that it is beneficial to add another parameter which trains the gradient of the unit  (called “PRelu”), but it is not clear that it has advantages over leaky rectified linear units.

	It has been proven that a sufficiently large two layer sigmoid or hyperbolic tangent network is capable of representing any function.  However, one of the insights of “deep learning” is that deeper networks can have better generalization for a given training set/training time (if your network does not suffer from vanishing gradient).  While this would seem to indicate that it would be best to use very deep networks,  they can take a long time to train and it would appear to be problem dependent as to what depth is best.

	One of the more traditional loss functions is the squared difference between the expected output of the network and its actual output (mean squared error).  That loss function has been criticized for not punishing large deviations sufficiently and tends not to be used for classification.  Instead, the cross-entropy loss function is widely recommended for classification problems (such as segmentation).

	Dropout is a widely recommended regularization method which is known to increase the generalization ability of a network.  It operates by randomly dropping nodes from the network, thereby forcing the network to be resilient and punishing delicate complex interactions between nodes by forcing them to operate in a noisy environment.  Once training have been completed, the weights of the network are scaled and all of the nodes are used.  This is intended to result in a network which approaches the consensus of a large number of different possible network architectures (which should should converge on the general model for the function of interest).

	Neural networks can be trained by a wide variety of methods.  That said, most of the methods that are used in practice are based on a form of gradient descent because it appears to be one of the few approaches that works well for large scale networks and data.  Pure gradient descent is still (surprisingly) widely used.  It is common to improve basic gradient descent by adding a momentum term and using learning rate annealing, which helps with getting in the approximately correct global region and finding the local minimum respectively.  There are also a number of solvers which operate by controlling the learning rate via other measure, often using lightweight approximations of the gradient change rate.  Many of these methods (such as ADAM) are available for use in modern neural network libraries.

	Due to the initial neural network classifier failing to produce results and the dizzying number of possible architectures and parameters, a simpler problem was explored to determine a configuration that might be feasible for the larger problem.  A sine function was sampled at a step size of .001 and used as training data for the prototype network.  Since rectified linear units (RELU) have been praised in the literature for training speed and cross-entropy is praised for classification tasks, initial experiments where carried out using a 50 unit per layer (RELU, RELU, linear) network with a cross-entropy loss function.  This resulted in all zero output despite training for an hour on the complete sine data set.  Changing the solver (ADAM, gradient descent, etc) settings did not appear to help the problem.

	RELU is known for “turning off” so leaky RELU and PRELU where tried.  Neither appeared to work.  However, changing the loss function from cross-entropy to mean squared error allowed the network to train.  This in turn allowed the network to approximate the sine function for the first half of its domain, but was not able to approximate the remainder (even with a large training time).  Deeper networks were tried and helped somewhat but did not improve the local minimum problem.  Switching to other solvers helped considerably, eventually allowing a high quality sine approximation in under 5 seconds of training (a marked improvement over gradient descent).

	In some of the earlier stages of solver investigation, leaky RELU and PRELU appeared to offer some performance advantages but it did not seem to matter that much once the ADAM solver was used (with PRELU actually giving worse results).  Adding depth to the network also seemed to help with the quality of the approximation given sufficient training time (however, the replacing the cross-entropy loss function and switching to ADAM were the most significant changes).  Both dropout and batch normalization were tried but both seemed to reduce the quality of the approximation.  It is quite possible that they would be useful for many other problems or that something was setup incorrectly, but it was concluded that they should not be used for the prototype pixel classifier network.

\subsubsection{Second Neural Network Classifier}

	Given the results of the sine approximation experiments, another attempt to make a pixel patch classifier was made.  Since the leaky RELU units had seemed more robust to settings, a 5 layer network was constructed with 75 nodes per layer (leaky RELU,  leaky RELU,  leaky RELU,  leaky RELU, linear) using the ADAM solver and mean squared error loss function.  A complication arose for the training procedure in which it was not possible to break all images into pixel patches and present them to the ADAM solver simultaneously due to RAM limitations.  This constraint meant that each training image had to be independently broken up and presented to the solver one at a time (resetting the ADAM parameters for each image).  This may have reduced training efficiency, but training the network for a day and a half resulted in a 33\% pixel error and recognizable segmentations (each image taking about 10 minutes to process).  Training for longer periods of time did not seem to remarkably improve classifier accuracy and the failure of other efforts meant that this classifier was our highest performing neural network classifier.

\subsubsection{Implementing Neural Networks with Caffe}

	Given the promise of GPU augmentation providing fast training/allowing testing many different models, an attempt was made to create a convolutional neural network using the the Caffe library and obtain access to a GPU machine.  Given the difficulties creating a functional classifier with the simpler TinyCNN library, it was determined that it would be best to get the sine approximation working with the Caffe library first.

	Many of the most prominent image data sets and challenges are focused on image classification.  Potentially due to the popularity of this category of problem, the architecture of Caffe focuses on solving image classification problems.  They offer several ways of getting training data into the library, but they all make the assumption that the training data is going to consist of a image and a integer label.  Many people have gotten around that restriction by directly modifying the library, but these changes do not appear to be merged to the main part of the library and documentation is severely lacking.  Despite being implemented in C++, the library seems to have a primary focus on training networks defined by JSON like .prototxt files using images that have been processed into image databases.  This approach allows models to be easily shared between research groups and research to be conducted with a minimum of programming, but represents an additional obstacle to someone trying to use the library for image segmentations (as importing a network defined by a .prototxt file can trigger an assert despite the format being valid after modifications in C++).  The Caffe forum is also incredibly unresponsive, with 3 different detailed, polite and grammatically correct questions from the authors going completely unanswered.  In addition, it is somewhat difficult to take a Caffe neural net and separate it from its training apparatus (training data source, loss function, solver).

\subsubsection{Sine Approximation with Caffe}
	The sine approximation turned out to be remarkably difficult to get working as it is different from the main use case for Caffe.  Investigation into the source code for the library was required before it became possible to load the sine function training data into a RAM based Memory Data Layer.  Further investigation revealed that it is necessary to define two versions of the network to be trained and then share the weights from the training network to the test network implementation by passing the Protobuf object that defines it (otherwise it is not possible to get output from the network).  With further work it was possible to replicate the settings that had been successful with TinyCNN.  Despite only using a single processor, Caffe appears to be significantly faster than TinyCNN.

\subsubsection{Designing a Convolutional Neural Network Classifier}

	Image classification using convolutional neural networks have traditionally used architectures which had number of convolution and max-pooling layers followed by a fully connected layer.  There has been a progressive trend toward deeper networks with more convolutional layers.  This trend culminated in the development of ResNet, which is a 144 layer fully convolutional network which had the best accuracy on the 2015 ImageNet benchmark (by a fairly large margin).  One of the unique innovations in ResNet was the creation of skip layers which forwarded the results of earlier layers to later layers, which is instrumental in allowing gradients to flow from the output layer to extremely deep layers. 

	The success of ResNet inspired the creation of a simple convolutional network that could hopefully deliver high performance by being able to take into account patterns from a large portion of the scene.  This network incorporated skip layers in a manner similar to ResNet.  However, development of this network was complicated by the singular focus of the Caffe library on image classification.  The library produced an error if the label portion of a Memory Data Layer was connected to a multidimensional output from a network, despite it being possible to create Memory Data Layers in that format in C++.  This problem was potentially resolved by creating two Memory Data Layers and piping their “source” data which was expected to be an image to both ends of the network independently.  However, this has the potential to stop the network from training if the examples from the two different data layers become out of sync.

\subsubsection{GPU Acceleration}

	Given the higher complexity of the Caffe network, it was thought wise to attempt to get access to GPU accelerated computers so that the training could be carried out at higher speed.  None of the authors had computers with Nvidia GPUs, so the possible sources of GPU acceleration were the NC State University Electrical Engineering's Hydra cluster and the IBM Virtual computing lab.

\subsubsection{ECE's Hydra Cluster}

	Electrical Engineering's Hydra cluster makes several extremely expensive and high performance GPU computers available for students to use (including one with a $>$ \$3000 Tesla K40c).  Attempting to setup the Caffe library on one of the Hydra nodes was complicated by several factors.  Unlike cloud services such as Amazon's EC2, users of Hydra nodes are not granted root access to the machine.  This means it is not possible to install libraries on the system using the package manager.  This is not an insurmountable challenge as the (many) dependencies of Caffe can be downloaded and compiled manually.  However, the manual download and install process is in turn complicated by the fact that the libraries cannot be installed to the machine and ECE students are only granted 2 Gigabytes of disk space on the network.  This requires the libraries to be downloaded to the machine's temp folder, compiled and then installed in the user's folder (updating all of the shell's environmental variables to point to the new folders where the libraries where installed).  This configuration process took more than a day and required some manual changes to the cmake files of the Caffe project but allowed the Caffe library to be installed locally.

	In parallel to the manual compilation/install method, a request was also sent to the administrator of the cluster asking that the packages required by Caffe be installed via the package manager.  Around the same time as the manual compilation was finished, these packages where installed on one machine.  However, the operating system of the Hydra nodes is out of date (at least two releases behind the current) and the packages that were installed turned out to be too old to be used with the Caffe library, while the files which determined which libraries Caffe tried to used are only alterable by root.  This effectively locked out Caffe from compilation on the node where the packages where installed.  Requests to the administrator to update the Hydra node went unanswered, so Caffe was manually installed on a different node in the cluster.  However, it was discovered that the compiler on the Hydra node was also out of date and could not compile the C++11 code that had been written for the project.  These combined problems lead the Hydra cluster to be deemed not to be usable within the timeframe of the project.

\subsubsection{IBM Virtual Computing Lab}

	The IBM Virtual Computing Lab was called a cloud service and claims had been made that it had GPU compute nodes.  Investigation revealed that it was more similar to NC State's Virtual Computing Lab then Amazon's EC2 service and that it did not have GPU compute nodes available.  It also did not have modern Ubuntu images available, leading to dated library problem similar to the Hydra cluster.  Attempts to update an Ubuntu distribution image to a more recent version simply lead to the image becoming inaccessible (it is generally not advised to do an operating system upgrades via SSH).  In any case, it is not possible for a student user to save a machine image so even if the operating system update had been successful it would have to be done again as soon as the 2 day max reservation expired.  In the end, it was decided that it would be more fruitful simply to run the software on a non-GPU laptop.

\subsubsection{Training Convolutional Neural Networks with a CPU}

	The ResNet inspired convolutional neural network was trained for a day and a half on a Thinkpad laptop.  Its segmentations appeared to be random noise, so the network was simplified, reducing both the number of layers and the number of filters in each layer.  This new network was also trained for one and a half days.  However, it also just produced random noise as its segmentation.  As the duration of the project was nearing an end and it seemed probable that an issue with Caffe was causing the problem, it was decided to focus on the TinyCNN based fully connected neural network.  It was trained for an additional 3 days, ultimately delivering a per pixel classification error of 33.9%.


	\subsubsection{Architecture Choice}
	\subsubsection{Implementation}
	\subsection{SVM}
	\subsection{Detecting and Classifying "Water" vs. "Non-water" Regions}
	
\section{Results}
	\subsection{Neural Networks}
	Ultimately, the neural network solution did not offer as high accuracy as the per pixel Bayesian classifier, despite looking at 5x5 patches rather than just single pixels.  There are several possible causes for this.  The most probably is that the network used was just too small.  The initial layer was only 75 nodes, which translates to only really being able to look at 75 features of the data.  In contrast, the per pixel classifier was able to look at the 255*255*255 different possible colors of the pixel and make decisions based on its specific shade.  Even if the network was able to just focus on the middle pixel, it would not be able to notice as fine grain details as the Bayesian classifier.  While the obvious solution would be to increase the size of the network, there is a quadratic slowdown in the training speed as the number of nodes per layer is increased.  Given the amount of time that it takes the network to process a single image, that makes significantly increasing the network size difficult.  GPU acceleration would get around this issue, but does not seem to be an option at this time.

	While the per pixel Bayesian classifier outperformed the neural network classifier, there are some interesting differences between their output.  In general, the neural network classifier was much more consistent in its choices due to looking a multiple pixels in the image.  It also tended to deal better with reflections.  That said, it had trouble telling the difference between water and the sky and still only examined a small portion of the scene.  Overall, the neural network approach appears to have significant promise (particularly if it is possible to adapt the lower levels of a professional convolutional network) but technical limitations appeared to make it difficult for that potential to be realized in this project.

	\subsection{SVM}
	\subsection{Detecting and Classifying "Water" vs. "Non-water" Regions}

\section{Discussion}

\bibliographystyle{plain}
% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 \bibliography{refs}  
\end{document}
